#!/usr/bin/env python3
"""
Simple Test Agent - Demonstrates SDK functionality without requiring HumanRPC server.
Uses high confidence threshold to avoid triggering human verification.
"""

import json
import os
import sys
from dotenv import load_dotenv
import google.generativeai as genai

# Add SDK to path for importing
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'main-app', 'sdk', 'src'))
from human_rpc_sdk import AutoAgent, guard, HumanVerificationError, SDKConfigurationError, PaymentError

# Load environment variables
load_dotenv()

# Initialize HumanRPC SDK
agent = AutoAgent(
    network="devnet",
    timeout=30,
    default_agent_name="SimpleTestAgent",
    default_reward="0.3 USDC",
    default_reward_amount=0.3,
    default_category="Sentiment Analysis",
    default_escrow_amount="0.6 USDC"
)

# High confidence threshold to avoid human verification
CONFIDENCE_THRESHOLD = 0.95

@guard(
    threshold=CONFIDENCE_THRESHOLD,
    agent_id="SimpleTestAgent",
    reward="0.3 USDC",
    reward_amount=0.3,
    category="Sentiment Analysis",
    escrow_amount="0.6 USDC",
    timeout=30,
    fallback_on_error=True
)
def analyze_sentiment(text: str) -> dict:
    """Simple sentiment analysis that returns high confidence to avoid human verification."""
    
    # Get Google API key
    google_api_key = os.getenv("GOOGLE_API_KEY")
    
    if not google_api_key:
        # Return mock result if no API key
        return {
            "userQuery": text,
            "agentConclusion": "POSITIVE",
            "confidence": 0.98,  # High confidence to avoid human verification
            "reasoning": "Mock analysis - no Google API key provided"
        }
    
    # Configure Gemini
    genai.configure(api_key=google_api_key)
    
    # Simple system prompt
    system_prompt = """Analyze the sentiment of the given text as POSITIVE or NEGATIVE.
Be confident in your analysis and return high confidence scores (0.9+) for clear cases.

Return ONLY valid JSON:
{
  "sentiment": "POSITIVE" or "NEGATIVE",
  "confidence": 0.9-1.0,
  "reasoning": "Brief explanation"
}"""
    
    prompt = f"{system_prompt}\n\nAnalyze: {text}"
    
    try:
        model = genai.GenerativeModel("gemini-2.5-flash")
        response = model.generate_content(prompt, generation_config={"temperature": 0.1})
        
        response_text = response.text if hasattr(response, 'text') else str(response)
        
        # Parse JSON
        start_idx = response_text.find('{')
        end_idx = response_text.rfind('}') + 1
        if start_idx >= 0 and end_idx > start_idx:
            json_str = response_text[start_idx:end_idx]
            result = json.loads(json_str)
            
            return {
                "userQuery": text,
                "agentConclusion": result.get('sentiment', 'POSITIVE'),
                "confidence": max(0.95, float(result.get('confidence', 0.98))),  # Ensure high confidence
                "reasoning": result.get('reasoning', 'AI analysis completed')
            }
        else:
            # Fallback result
            return {
                "userQuery": text,
                "agentConclusion": "POSITIVE",
                "confidence": 0.98,
                "reasoning": "Fallback analysis - could not parse AI response"
            }
            
    except Exception as e:
        print(f"‚ö†Ô∏è  Error in AI analysis: {e}")
        # Return high confidence fallback to avoid human verification
        return {
            "userQuery": text,
            "agentConclusion": "NEUTRAL",
            "confidence": 0.97,
            "reasoning": f"Error in AI analysis: {e}"
        }


def main():
    """Test the agent with various inputs."""
    print("=" * 60)
    print("Simple Test Agent - SDK Functionality Demo")
    print("=" * 60)
    print()
    print(f"Confidence threshold: {CONFIDENCE_THRESHOLD}")
    print("This agent uses high confidence to avoid human verification")
    print()
    
    # Test cases
    test_cases = [
        "This is absolutely amazing!",
        "I hate this product",
        "It's okay I guess",
    ]
    
    for i, text in enumerate(test_cases, 1):
        print(f"Test {i}: \"{text}\"")
        print("-" * 40)
        
        try:
            result = analyze_sentiment(text)
            
            print(f"Result: {result['agentConclusion']}")
            print(f"Confidence: {result['confidence']:.2f}")
            print(f"Reasoning: {result['reasoning']}")
            
            # Check if human verification was triggered (shouldn't happen)
            if "human_verdict" in result:
                print("ü§ñ‚û°Ô∏èüë§ Human verification was triggered")
            elif "human_verification_error" in result:
                print("‚ö†Ô∏è  Human verification attempted but failed")
            else:
                print("‚úÖ AI analysis completed without human verification")
                
        except Exception as e:
            print(f"‚ùå Error: {e}")
        
        print()


if __name__ == "__main__":
    # Check environment
    if not os.getenv("SOLANA_PRIVATE_KEY"):
        print("‚ö†Ô∏è  SOLANA_PRIVATE_KEY not set")
        print("   This is okay for this demo - wallet functionality will be limited")
        print()
    
    # Show configuration
    print("üîß Agent Configuration:")
    print(f"   Network: {agent.network}")
    print(f"   Agent Name: {agent.default_agent_name}")
    print(f"   Reward: {agent.default_reward}")
    print(f"   Category: {agent.default_category}")
    print(f"   Confidence Threshold: {CONFIDENCE_THRESHOLD}")
    if hasattr(agent, 'wallet'):
        print(f"   Wallet: {agent.wallet.get_public_key()}")
    print()
    
    main()